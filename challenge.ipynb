{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-email",
   "metadata": {},
   "source": [
    "# Prueba Técnica Data Engineer - Global Mobility Apex\n",
    "\n",
    "**Origen:** Este challenge fue enviado por Sebastián Rincon Agredo (jrincon@apexglobal.co) a Ronald Castillo Capino el 6 de marzo de 2025.\n",
    "\n",
    "**Contexto:**\n",
    "\n",
    "- Se trata de la prueba técnica para la vacante de Data Engineer en Global Mobility Apex.\n",
    "- Los entregables deberán ser enviados por Get on Board.\n",
    "\n",
    "Esta prueba consiste en desarrollar un dashboard de ventas que incluya:\n",
    "\n",
    "- Procesamiento y limpieza de datos de ventas a partir de un CSV con algunas irregularidades.\n",
    "- Transformaciones y cálculos de métricas agregadas (precio promedio, ingreso total, día de mayor venta, etc.).\n",
    "- Detección de outliers basados en la cantidad de cada transacción.\n",
    "- Almacenamiento de los datos procesados en una base de datos SQLite con tablas separadas para transacciones, métricas agregadas y outliers.\n",
    "- Un API RESTful (utilizando FastAPI) que exponga endpoints para consultar:\n",
    "  - Ventas totales por producto (con filtros opcionales por nombre de producto o categoría).\n",
    "  - Ventas totales por día (con opción de filtrar por rango de fechas).\n",
    "  - Métricas agregadas por categoría.\n",
    "  - Transacciones marcadas como outliers.\n",
    "\n",
    "A continuación se presenta el código completo que cumple con estos requerimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Para la parte del API\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "print('Librerías importadas correctamente.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-processing-intro",
   "metadata": {},
   "source": [
    "## 1. Procesamiento y Limpieza de Datos\n",
    "\n",
    "En esta sección se realiza la ingesta y limpieza del archivo CSV que contiene los datos de ventas. Se deben considerar los siguientes puntos:\n",
    "\n",
    "- **Valores faltantes en `quantity`**: Se reemplazan por 0.\n",
    "- **Valores inválidos en `price`**: Los valores que no se puedan convertir a número (por ejemplo, \"not_a_number\") se reemplazarán por la mediana del precio de la misma categoría.\n",
    "- Se calcula la columna `total_sales` (multiplicando `quantity` y `price`).\n",
    "- Se crea la columna `day_of_week` a partir de la fecha, y un flag `high_volume` que marca como `True` aquellas transacciones con `quantity` mayor a 10.\n",
    "\n",
    "Estos pasos aseguran que la información esté limpia y lista para las transformaciones posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Simular el CSV usando un string (en un caso real, se leería de un archivo, por ejemplo, pd.read_csv('sales_data.csv'))\n",
    "csv_data = '''transaction_id,date,category,product,quantity,price\n",
    "1,2024-07-01,Widget,Widget-A,10,9.99\n",
    "2,2024-07-01,Gadget,Gadget-X,5,19.99\n",
    "3,2024-07-02,Widget,Widget-B,7,9.99\n",
    "4,2024-07-02,Doodad,Doodad-1,,4.99\n",
    "5,2024-07-03,Widget,Widget-C,3,9.99\n",
    "6,2024-07-03,Gadget,Gadget-Y,8,19.99\n",
    "7,2024-07-04,Widget,Widget-A,2,9.99\n",
    "8,2024-07-04,Doodad,Doodad-2,4,not_a_number\n",
    "9,2024-07-05,Widget,Widget-B,6,9.99\n",
    "10,2024-07-05,Gadget,Gadget-X,3,19.99\n",
    "11,2024-07-06,Gadget,,5,19.99\n",
    "12,2024-07-06,Doodad,Doodad-3,1,4.99\n",
    "13,2024-07-07,Widget,Widget-C,8,9.99\n",
    "14,2024-07-07,Gadget,Gadget-Y,4,19.99\n",
    "15,2024-07-08,Widget,Widget-A,3,9.99\n",
    "16,2024-07-08,Doodad,Doodad-1,2,4.99\n",
    "17,2024-07-09,Gadget,Gadget-X,,19.99\n",
    "18,2024-07-09,Widget,Widget-B,5,9.99\n",
    "19,2024-07-10,Doodad,Doodad-2,4,4.99\n",
    "20,2024-07-10,Gadget,Gadget-Y,7,19.99\n",
    "21,2024-07-11,Widget,Widget-C,6,9.99\n",
    "22,2024-07-11,Doodad,Doodad-3,3,4.99\n",
    "23,2024-07-12,Gadget,Gadget-X,9,19.99\n",
    "24,2024-07-12,Widget,Widget-A,1,9.99\n",
    "25,2024-07-13,Doodad,Doodad-1,2,4.99\n",
    "26,2024-07-13,Gadget,Gadget-Y,5,19.99\n",
    "27,2024-07-14,Widget,Widget-B,,9.99\n",
    "28,2024-07-14,Doodad,Doodad-2,4,4.99\n",
    "29,2024-07-15,Gadget,Gadget-X,7,19.99\n",
    "30,2024-07-15,Widget,Widget-C,3,9.99\n",
    "31,2024-07-16,Doodad,Doodad-3,1,4.99\n",
    "32,2024-07-16,Gadget,Gadget-Y,5,not_a_number\n",
    "33,2024-07-17,Widget,Widget-A,8,9.99\n",
    "34,2024-07-17,Doodad,Doodad-1,2,4.99\n",
    "35,2024-07-18,Gadget,Gadget-X,6,19.99\n",
    "36,2024-07-18,Widget,Widget-B,4,9.99\n",
    "37,2024-07-19,Doodad,Doodad-2,3,4.99\n",
    "38,2024-07-19,Gadget,Gadget-Y,2,19.99\n",
    "39,2024-07-20,Widget,Widget-C,5,9.99\n",
    "40,2024-07-20,Doodad,Doodad-3,,4.99\n",
    "41,2024-07-21,Gadget,Gadget-X,7,19.99\n",
    "42,2024-07-21,Widget,Widget-A,3,9.99\n",
    "43,2024-07-22,Doodad,Doodad-1,2,4.99\n",
    "44,2024-07-22,Gadget,Gadget-Y,6,19.99\n",
    "45,2024-07-23,Widget,Widget-B,7,9.99\n",
    "46,2024-07-23,Doodad,Doodad-2,3,4.99\n",
    "47,2024-07-24,Gadget,Gadget-X,,19.99\n",
    "48,2024-07-24,Widget,Widget-C,5,9.99\n",
    "49,2024-07-25,Doodad,Doodad-3,4,4.99\n",
    "50,2024-07-25,Gadget,Gadget-Y,8,19.99\n",
    "''' \n",
    "\n",
    "# Leer el CSV\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "print('Datos originales:')\n",
    "print(df.head(10))\n",
    "\n",
    "# Reemplazar valores faltantes en 'quantity' por 0\n",
    "df['quantity'] = df['quantity'].fillna(0)\n",
    "\n",
    "# Convertir 'quantity' a numérico\n",
    "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Convertir 'price' a numérico; los errores se convierten a NaN\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "# Función para reemplazar precios inválidos por la mediana del grupo\n",
    "def replace_invalid_prices(group):\n",
    "    median_price = group['price'].median()\n",
    "    group['price'] = group['price'].fillna(median_price)\n",
    "    return group\n",
    "\n",
    "df = df.groupby('category', group_keys=False).apply(replace_invalid_prices)\n",
    "\n",
    "# Calcular la columna total_sales\n",
    "df['total_sales'] = df['quantity'] * df['price']\n",
    "\n",
    "# Convertir la columna 'date' a tipo datetime y extraer el día de la semana\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "# Flag high_volume: True si quantity > 10\n",
    "df['high_volume'] = df['quantity'] > 10\n",
    "\n",
    "print('\\nDatos procesados:')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-transformations",
   "metadata": {},
   "source": [
    "## 2. Transformaciones Complejas y Cálculo de Métricas Agregadas\n",
    "\n",
    "Se realizan las siguientes transformaciones:\n",
    "\n",
    "- **Métricas Agregadas:**\n",
    "  - Precio promedio por producto (media de `price`) en cada categoría.\n",
    "  - Ingreso total (suma de `total_sales`) por categoría.\n",
    "  - Día de la semana con mayor total de ventas por categoría.\n",
    "\n",
    "- **Detección de Outliers:**\n",
    "  - Se identifican aquellas transacciones cuyo `quantity` es mayor a 2 desviaciones estándar respecto de la media de la categoría. Se marca con un flag `outlier`.\n",
    "\n",
    "Esto permite obtener una visión más completa del desempeño de cada categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregations-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupación por categoría para obtener métricas\n",
    "category_group = df.groupby('category')\n",
    "\n",
    "agg_metrics = category_group.agg(\n",
    "    avg_price=('price', 'mean'),\n",
    "    total_revenue=('total_sales', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Para el día con mayor ventas por categoría\n",
    "day_sales = df.groupby(['category', 'day_of_week'])['total_sales'].sum().reset_index()\n",
    "\n",
    "# Encontrar el día con el máximo total de ventas por categoría\n",
    "max_day = day_sales.loc[day_sales.groupby('category')['total_sales'].idxmax()].rename(columns={'day_of_week': 'top_day'})\n",
    "\n",
    "agg_metrics = pd.merge(agg_metrics, max_day[['category', 'top_day']], on='category')\n",
    "\n",
    "print('Métricas Agregadas por Categoría:')\n",
    "print(agg_metrics)\n",
    "\n",
    "# Detección de outliers en 'quantity' por categoría\n",
    "def detect_outliers(group):\n",
    "    mean_q = group['quantity'].mean()\n",
    "    std_q = group['quantity'].std()\n",
    "    group['outlier'] = (group['quantity'] > (mean_q + 2 * std_q))\n",
    "    return group\n",
    "\n",
    "df = df.groupby('category', group_keys=False).apply(detect_outliers)\n",
    "\n",
    "outliers = df[df['outlier']]\n",
    "\n",
    "print('\\nOutliers detectados:')\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sqlite-storage",
   "metadata": {},
   "source": [
    "## 3. Almacenamiento en SQLite\n",
    "\n",
    "Se crean tres tablas en la base de datos SQLite:\n",
    "\n",
    "- **transactions:** Datos de las transacciones procesadas.\n",
    "- **aggregated_metrics:** Métricas agregadas por categoría.\n",
    "- **outliers:** Transacciones marcadas como outliers.\n",
    "\n",
    "Esto permite almacenar la información para consultas eficientes y servirla a través del API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqlite-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear/conectar a la base de datos SQLite\n",
    "db_name = 'sales_dashboard.db'\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# Guardar las tablas en la base de datos\n",
    "df.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "agg_metrics.to_sql('aggregated_metrics', conn, if_exists='replace', index=False)\n",
    "outliers.to_sql('outliers', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(f'Datos guardados en la base de datos {db_name}')\n",
    "\n",
    "# Cerrar la conexión (se reabrirá en el API si es necesario)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-intro",
   "metadata": {},
   "source": [
    "## 4. Backend: API RESTful con FastAPI\n",
    "\n",
    "En esta sección se implementa un API RESTful que expone los siguientes endpoints:\n",
    "\n",
    "1. **GET /sales/product**: Retorna el total de ventas por producto. Se puede filtrar opcionalmente por nombre de producto o categoría.\n",
    "2. **GET /sales/day**: Retorna el total de ventas por día, con opción de filtrar por un rango de fechas.\n",
    "3. **GET /sales/category**: Retorna las métricas agregadas por categoría (total revenue, average price, día con mayor ventas).\n",
    "4. **GET /sales/outliers**: Retorna las transacciones marcadas como outliers.\n",
    "\n",
    "El API se conecta a la base de datos SQLite creada previamente para consultar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title='Sales Dashboard API')\n",
    "\n",
    "DATABASE = 'sales_dashboard.db'\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DATABASE)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "@app.get('/sales/product')\n",
    "def get_sales_by_product(product: Optional[str] = None, category: Optional[str] = None):\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT product, SUM(total_sales) as total_sales FROM transactions\"\n",
    "    conditions = []\n",
    "    params = []\n",
    "    if product:\n",
    "        conditions.append(\"product = ?\")\n",
    "        params.append(product)\n",
    "    if category:\n",
    "        conditions.append(\"category = ?\")\n",
    "        params.append(category)\n",
    "    if conditions:\n",
    "        query += \" WHERE \" + \" AND \".join(conditions)\n",
    "    query += \" GROUP BY product\"\n",
    "    \n",
    "    cur = conn.execute(query, params)\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    results = [dict(row) for row in rows]\n",
    "    return JSONResponse(content=results)\n",
    "\n",
    "\n",
    "@app.get('/sales/day')\n",
    "def get_sales_by_day(start_date: Optional[str] = None, end_date: Optional[str] = None):\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT date(date) as date, SUM(total_sales) as total_sales FROM transactions\"\n",
    "    conditions = []\n",
    "    params = []\n",
    "    if start_date:\n",
    "        conditions.append(\"date >= ?\")\n",
    "        params.append(start_date)\n",
    "    if end_date:\n",
    "        conditions.append(\"date <= ?\")\n",
    "        params.append(end_date)\n",
    "    if conditions:\n",
    "        query += \" WHERE \" + \" AND \".join(conditions)\n",
    "    query += \" GROUP BY date\"\n",
    "    \n",
    "    cur = conn.execute(query, params)\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    results = [dict(row) for row in rows]\n",
    "    return JSONResponse(content=results)\n",
    "\n",
    "\n",
    "@app.get('/sales/category')\n",
    "def get_sales_by_category():\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT * FROM aggregated_metrics\"\n",
    "    cur = conn.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    results = [dict(row) for row in rows]\n",
    "    return JSONResponse(content=results)\n",
    "\n",
    "\n",
    "@app.get('/sales/outliers')\n",
    "def get_outliers():\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT * FROM outliers\"\n",
    "    cur = conn.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    results = [dict(row) for row in rows]\n",
    "    return JSONResponse(content=results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-testing",
   "metadata": {},
   "source": [
    "### Ejecución y Prueba del API\n",
    "\n",
    "Para ejecutar el API desde el notebook sin provocar errores de asyncio, se utiliza la librería `nest_asyncio`.\n",
    "\n",
    "1. Asegúrate de instalar `nest_asyncio`:\n",
    "   ```bash\n",
    "   pip install nest_asyncio\n",
    "   ```\n",
    "2. Ejecuta la celda siguiente para iniciar el servidor en `http://localhost:8000`.\n",
    "\n",
    "También puedes ejecutar el script fuera del notebook en modo terminal si lo prefieres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-api",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar nest_asyncio para evitar conflictos con el loop de eventos en Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print('Iniciando el servidor FastAPI en http://localhost:8000')\n",
    "uvicorn.run(app, host='0.0.0.0', port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
